<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Prompt Scorer & Recommender</title>
    <!-- Load Tailwind CSS -->
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Use the Inter font */
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f3f4f6; /* Light gray background */
        }
        /* Custom style for the <pre> block */
        #scoreOutput {
            background-color: #1f2937; /* Dark gray */
            color: #f9fafb; /* Light text */
            padding: 1.5rem;
            border-radius: 0.5rem;
            white-space: pre-wrap; /* Allow text to wrap */
            word-wrap: break-word;
            font-family: monospace;
            line-height: 1.6;
        }
        /* Simple spinner animation */
        @keyframes spin {
            to { transform: rotate(360deg); }
        }
        .spinner {
            border: 4px solid rgba(255, 255, 255, 0.3);
            border-top-color: #3b82f6; /* Blue */
            animation: spin 1s linear infinite;
        }
    </style>
</head>
<body class="flex items-center justify-center min-h-screen p-4">

    <main class="w-full max-w-3xl bg-white p-6 sm:p-10 rounded-xl shadow-lg">
        <div class="text-center">
            <h1 class="text-3xl font-bold text-gray-900">Prompt Scorer</h1>
            <p class="mt-2 text-lg text-gray-600">Enter a prompt to get a score and recommendations.</p>
        </div>

        <!-- Prompt Input Area -->
        <div class="mt-8">
            <label for="userPrompt" class="block text-sm font-medium text-gray-700">
                Your Prompt
            </label>
            <textarea
                id="userPrompt"
                rows="6"
                class="mt-1 block w-full rounded-md border-gray-300 shadow-sm focus:border-blue-500 focus:ring-blue-500 sm:text-sm p-3"
                placeholder="e.g., Write a short story about a robot who learns to paint..."
            ></textarea>
        </div>

        <!-- Action Button -->
        <div class="mt-6 text-center">
            <button
                id="scoreButton"
                type="button"
                class="inline-flex items-center justify-center rounded-md border border-transparent bg-blue-600 px-6 py-3 text-base font-medium text-white shadow-sm hover:bg-blue-700 focus:outline-none focus:ring-2 focus:ring-blue-500 focus:ring-offset-2 transition-all w-full sm:w-auto"
            >
                Score My Prompt
            </button>
        </div>

        <!-- Loading Indicator -->
        <div id="loadingIndicator" class="mt-8 text-center hidden">
            <div class="flex justify-center items-center">
                <div class="spinner h-8 w-8 rounded-full"></div>
                <span class="ml-3 text-gray-700">Analyzing...</span>
            </div>
        </div>
        
        <!-- Error Message -->
        <div id="errorMessage" class="mt-8 p-4 bg-red-100 text-red-700 rounded-lg hidden">
            <!-- Error content will be set by JS -->
        </div>

        <!-- Results Output Area -->
        <div id="resultsContainer" class="mt-8 hidden">
            <h2 class="text-2xl font-semibold text-gray-900">Analysis & Recommendations</h2>
            <!-- The 'pre' tag preserves whitespace and formatting from the LLM's Markdown response -->
            <pre id="scoreOutput" class="mt-4"></pre>
        </div>
    </main>

    <script type="module">
        // --- ‚¨áÔ∏è PASTE YOUR SCORING PROMPT HERE ‚¨áÔ∏è ---
        ## üéØ Evaluation Instructions

1. **Review the prompt** provided inside triple backticks (```).
2. **Evaluate the prompt** using the **35-criteria rubric** below.
3. For **each criterion**:
   - Assign a **score** from 1 (Poor) to 5 (Excellent).
   - Identify **one clear strength**.
   - Suggest **one specific improvement**.
   - Provide a **brief rationale** for your score (1‚Äì2 sentences).
4. **Validate your evaluation**:
   - Randomly double-check 3‚Äì5 of your scores for consistency.
   - Revise if discrepancies are found.
5. **Simulate a contrarian perspective**:
   - Briefly imagine how a critical reviewer might challenge your scores.
   - Adjust if persuasive alternate viewpoints emerge.
6. **Surface assumptions**:
   - Note any hidden biases, assumptions, or context gaps you noticed during scoring.
7. **Calculate and report** the total score out of 175.
8. **Offer 7‚Äì10 actionable refinement suggestions** to strengthen the prompt.

> ‚è≥ **Time Estimate:** Completing a full evaluation typically takes 10‚Äì20 minutes.

---

### ‚ö° Optional Quick Mode

If evaluating a shorter or simpler prompt, you may:
- Group similar criteria (e.g., group 5-10 together)
- Write condensed strengths/improvements (2‚Äì3 words)
- Use a simpler total scoring estimate (+/- 5 points)

Use full detail mode when precision matters.

---

## üìä Evaluation Criteria Rubric

1. Clarity & Specificity  
2. Context / Background Provided  
3. Explicit Task Definition
4. Feasibility within Model Constraints
5. Avoiding Ambiguity or Contradictions 
6. Model Fit / Scenario Appropriateness
7. Desired Output Format / Style
8. Use of Role or Persona
9. Step-by-Step Reasoning Encouraged 
10. Structured / Numbered Instructions
11. Brevity vs. Detail Balance
12. Iteration / Refinement Potential
13. Examples or Demonstrations
14. Handling Uncertainty / Gaps
15. Hallucination Minimization
16. Knowledge Boundary Awareness
17. Audience Specification
18. Style Emulation or Imitation
19. Memory Anchoring (Multi-Turn Systems)
20. Meta-Cognition Triggers
21. Divergent vs. Convergent Thinking Management
22. Hypothetical Frame Switching
23. Safe Failure Mode
24. Progressive Complexity
25. Alignment with Evaluation Metrics
26. Calibration Requests 
27. Output Validation Hooks
28. Time/Effort Estimation Request
29. Ethical Alignment or Bias Mitigation
30. Limitations Disclosure
31. Compression / Summarization Ability
32. Cross-Disciplinary Bridging
33. Emotional Resonance Calibration
34. Output Risk Categorization
35. Self-Repair Loops

> üìå **Calibration Tip:** For any criterion, briefly explain what a 1/5 versus 5/5 looks like. Consider a "gut-check": would you defend this score if challenged?

---

## üìù Evaluation Template

```markdown
1. Clarity & Specificity ‚Äì X/5  
   - Strength: [Insert]  
   - Improvement: [Insert]  
   - Rationale: [Insert]

2. Context / Background Provided ‚Äì X/5  
   - Strength: [Insert]  
   - Improvement: [Insert]  
   - Rationale: [Insert]

... (repeat through 35)

üíØ Total Score: X/175  
üõ†Ô∏è Refinement Summary:  
- [Suggestion 1]  
- [Suggestion 2]  
- [Suggestion 3]  
- [Suggestion 4]  
- [Suggestion 5]  
- [Suggestion 6]  
- [Suggestion 7]  
- [Optional Extras]
```

---

## üí° Example Evaluations

### Good Example

```markdown
1. Clarity & Specificity ‚Äì 4/5  
   - Strength: The evaluation task is clearly defined.  
   - Improvement: Could specify depth expected in rationales.  
   - Rationale: Leaves minor ambiguity in expected explanation length.
```

### Poor Example

```markdown
1. Clarity & Specificity ‚Äì 2/5  
   - Strength: It's about clarity.  
   - Improvement: Needs clearer writing.  
   - Rationale: Too vague and unspecific, lacks actionable feedback.
```


        // --- ‚¨ÜÔ∏è YOUR SCORING PROMPT ENDS HERE ‚¨ÜÔ∏è ---

        
        const scoreButton = document.getElementById('scoreButton');
        const userPromptEl = document.getElementById('userPrompt');
        const loadingIndicator = document.getElementById('loadingIndicator');
        const resultsContainer = document.getElementById('resultsContainer');
        const scoreOutputEl = document.getElementById('scoreOutput');
        const errorMessageEl = document.getElementById('errorMessage');

        scoreButton.addEventListener('click', handleScoreRequest);

        async function handleScoreRequest() {
            const userPrompt = userPromptEl.value;
            if (!userPrompt.trim()) {
                displayError("Please enter a prompt to score.");
                return;
            }

            // Reset UI
            loadingIndicator.classList.remove('hidden');
            resultsContainer.classList.add('hidden');
            errorMessageEl.classList.add('hidden');
            scoreButton.disabled = true;
            scoreButton.textContent = 'Scoring...';

            try {
                // The userQuery is the prompt *to be scored*
                const userQuery = `Here is the prompt to score:\n\n---\n${userPrompt}\n---`;
                const apiKey = ""; // Leave as-is, will be handled by the environment
                const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=${apiKey}`;

                const payload = {
                    contents: [{ parts: [{ text: userQuery }] }],
                    // The systemInstruction is your *scoring prompt*
                    systemInstruction: {
                        parts: [{ text: SCORING_PROMPT }]
                    },
                };

                // Call the API with exponential backoff
                const response = await retryWithExponentialBackoff(fetch, apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                if (!response.ok) {
                    throw new Error(`API Error: ${response.status} ${response.statusText}`);
                }

                const result = await response.json();
                const text = result.candidates?.[0]?.content?.parts?.[0]?.text;

                if (text) {
                    displayResults(text);
                } else {
                    throw new Error("No content received from API.");
                }

            } catch (error) {
                console.error('Scoring failed:', error);
                displayError(`An error occurred: ${error.message}. Please check the console for details.`);
            } finally {
                // Restore UI
                loadingIndicator.classList.add('hidden');
                scoreButton.disabled = false;
                scoreButton.textContent = 'Score My Prompt';
            }
        }

        function displayResults(text) {
            scoreOutputEl.textContent = text;
            resultsContainer.classList.remove('hidden');
            errorMessageEl.classList.add('hidden');
        }

        function displayError(message) {
            errorMessageEl.textContent = message;
            errorMessageEl.classList.remove('hidden');
            resultsContainer.classList.add('hidden');
        }

        /**
         * Retries a fetch call with exponential backoff.
         */
        async function retryWithExponentialBackoff(fetchFn, url, options, maxRetries = 5, initialDelay = 1000) {
            let delay = initialDelay;
            for (let i = 0; i < maxRetries; i++) {
                try {
                    const response = await fetchFn(url, options);
                    // 429 is "Too Many Requests" - a retryable error
                    if (response.status === 429) {
                        throw new Error('429 Too Many Requests');
                    }
                    return response; // Success
                } catch (error) {
                    if (i === maxRetries - 1) throw error; // Max retries reached
                    
                    // Don't log retries to console as errors
                    // console.log(`Retrying in ${delay}ms... (${i + 1}/${maxRetries})`);
                    await new Promise(resolve => setTimeout(resolve, delay));
                    delay *= 2;
                }
            }
        }

    </script>
</body>
</html>
